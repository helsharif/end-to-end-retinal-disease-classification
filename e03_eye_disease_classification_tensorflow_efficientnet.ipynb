{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58ac3c22-1550-4021-a8ac-ba77f53ddaaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Husayn El Sharif\n",
    "comment = \"\"\"\n",
    "Use environment: env003 which has tensorflow support\n",
    "use efficientnet from tensorhub and force 456 x 456 size\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3371f3aa-3545-4237-9881-1f1dc84fbb9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-04 12:10:40.409998: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2026-01-04 12:10:40.445155: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2026-01-04 12:10:41.451532: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF Version: 2.20.0\n",
      "TF Hub Version: 0.16.1\n",
      "tf.keras module path: /home/helsharif/miniconda3/envs/env003/lib/python3.11/site-packages/tf_keras/api/_v2/keras/__init__.py\n",
      "GPUs available:\n",
      "- PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/helsharif/miniconda3/envs/env003/lib/python3.11/site-packages/tensorflow_hub/__init__.py:61: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import parse_version\n"
     ]
    }
   ],
   "source": [
    "# TensorFlow Import and GPU Check\n",
    "\n",
    "import os\n",
    "os.environ[\"TF_USE_LEGACY_KERAS\"] = \"1\"\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "\n",
    "\n",
    "print(\"TF Version:\", tf.__version__)\n",
    "print(\"TF Hub Version:\", hub.__version__)\n",
    "print(\"tf.keras module path:\", tf.keras.__file__)\n",
    "\n",
    "# Check if GPU is savailable\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    print(\"GPUs available:\")\n",
    "    for gpu in gpus:\n",
    "        print(f\"- {gpu}\")\n",
    "else:\n",
    "    print(\"No GPUs found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02e494ad-c819-4020-8163-c9affc2e1a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# other imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "import datetime\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "from IPython.display import Image\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "import datetime\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import classification_report\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f5b896b-1a7e-4c3b-8e7b-6311b9d30d7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "image_filepath",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "label",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "patient_id",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "51be5f05-cd18-4197-af5a-e2a7654b91f7",
       "rows": [
        [
         "0",
         "images/cataract/0_4015166.jpg",
         "cataract",
         "0_cataract"
        ],
        [
         "1",
         "images/cataract/0_left.jpg",
         "cataract",
         "0_cataract"
        ],
        [
         "2",
         "images/cataract/100_334408.jpg",
         "cataract",
         "100_cataract"
        ],
        [
         "3",
         "images/cataract/101_5139882.jpg",
         "cataract",
         "101_cataract"
        ],
        [
         "4",
         "images/cataract/101_709333.jpg",
         "cataract",
         "101_cataract"
        ],
        [
         "5",
         "images/cataract/102_1024926.jpg",
         "cataract",
         "102_cataract"
        ],
        [
         "6",
         "images/cataract/102_3990401.jpg",
         "cataract",
         "102_cataract"
        ],
        [
         "7",
         "images/cataract/103_5897328.jpg",
         "cataract",
         "103_cataract"
        ],
        [
         "8",
         "images/cataract/103_left.jpg",
         "cataract",
         "103_cataract"
        ],
        [
         "9",
         "images/cataract/104_4607170.jpg",
         "cataract",
         "104_cataract"
        ],
        [
         "10",
         "images/cataract/105_5646774.jpg",
         "cataract",
         "105_cataract"
        ],
        [
         "11",
         "images/cataract/105_5956883.jpg",
         "cataract",
         "105_cataract"
        ],
        [
         "12",
         "images/cataract/1062_right.jpg",
         "cataract",
         "1062_cataract"
        ],
        [
         "13",
         "images/cataract/106_5284713.jpg",
         "cataract",
         "106_cataract"
        ],
        [
         "14",
         "images/cataract/107_5060742.jpg",
         "cataract",
         "107_cataract"
        ],
        [
         "15",
         "images/cataract/1083_left.jpg",
         "cataract",
         "1083_cataract"
        ],
        [
         "16",
         "images/cataract/1084_right.jpg",
         "cataract",
         "1084_cataract"
        ],
        [
         "17",
         "images/cataract/108_750835.jpg",
         "cataract",
         "108_cataract"
        ],
        [
         "18",
         "images/cataract/108_8917257.jpg",
         "cataract",
         "108_cataract"
        ],
        [
         "19",
         "images/cataract/109_2006348.jpg",
         "cataract",
         "109_cataract"
        ],
        [
         "20",
         "images/cataract/109_2655056.jpg",
         "cataract",
         "109_cataract"
        ],
        [
         "21",
         "images/cataract/10_3215668.jpg",
         "cataract",
         "10_cataract"
        ],
        [
         "22",
         "images/cataract/1102_left.jpg",
         "cataract",
         "1102_cataract"
        ],
        [
         "23",
         "images/cataract/1102_right.jpg",
         "cataract",
         "1102_cataract"
        ],
        [
         "24",
         "images/cataract/110_5076150.jpg",
         "cataract",
         "110_cataract"
        ],
        [
         "25",
         "images/cataract/1115_left.jpg",
         "cataract",
         "1115_cataract"
        ],
        [
         "26",
         "images/cataract/111_9826667.jpg",
         "cataract",
         "111_cataract"
        ],
        [
         "27",
         "images/cataract/1126_right.jpg",
         "cataract",
         "1126_cataract"
        ],
        [
         "28",
         "images/cataract/112_3727502.jpg",
         "cataract",
         "112_cataract"
        ],
        [
         "29",
         "images/cataract/112_9621689.jpg",
         "cataract",
         "112_cataract"
        ],
        [
         "30",
         "images/cataract/112_right.jpg",
         "cataract",
         "112_cataract"
        ],
        [
         "31",
         "images/cataract/113_1734230.jpg",
         "cataract",
         "113_cataract"
        ],
        [
         "32",
         "images/cataract/113_2699140.jpg",
         "cataract",
         "113_cataract"
        ],
        [
         "33",
         "images/cataract/1144_left.jpg",
         "cataract",
         "1144_cataract"
        ],
        [
         "34",
         "images/cataract/1144_right.jpg",
         "cataract",
         "1144_cataract"
        ],
        [
         "35",
         "images/cataract/114_3613830.jpg",
         "cataract",
         "114_cataract"
        ],
        [
         "36",
         "images/cataract/114_5711178.jpg",
         "cataract",
         "114_cataract"
        ],
        [
         "37",
         "images/cataract/114_9773152.jpg",
         "cataract",
         "114_cataract"
        ],
        [
         "38",
         "images/cataract/115_1763401.jpg",
         "cataract",
         "115_cataract"
        ],
        [
         "39",
         "images/cataract/115_6901151.jpg",
         "cataract",
         "115_cataract"
        ],
        [
         "40",
         "images/cataract/1164_left.jpg",
         "cataract",
         "1164_cataract"
        ],
        [
         "41",
         "images/cataract/1167_right.jpg",
         "cataract",
         "1167_cataract"
        ],
        [
         "42",
         "images/cataract/116_4683353.jpg",
         "cataract",
         "116_cataract"
        ],
        [
         "43",
         "images/cataract/117_6521529.jpg",
         "cataract",
         "117_cataract"
        ],
        [
         "44",
         "images/cataract/118_6296686.jpg",
         "cataract",
         "118_cataract"
        ],
        [
         "45",
         "images/cataract/119_6929742.jpg",
         "cataract",
         "119_cataract"
        ],
        [
         "46",
         "images/cataract/119_9887385.jpg",
         "cataract",
         "119_cataract"
        ],
        [
         "47",
         "images/cataract/119_left.jpg",
         "cataract",
         "119_cataract"
        ],
        [
         "48",
         "images/cataract/11_7158929.jpg",
         "cataract",
         "11_cataract"
        ],
        [
         "49",
         "images/cataract/120_2375814.jpg",
         "cataract",
         "120_cataract"
        ]
       ],
       "shape": {
        "columns": 3,
        "rows": 4016
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_filepath</th>\n",
       "      <th>label</th>\n",
       "      <th>patient_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>images/cataract/0_4015166.jpg</td>\n",
       "      <td>cataract</td>\n",
       "      <td>0_cataract</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>images/cataract/0_left.jpg</td>\n",
       "      <td>cataract</td>\n",
       "      <td>0_cataract</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>images/cataract/100_334408.jpg</td>\n",
       "      <td>cataract</td>\n",
       "      <td>100_cataract</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>images/cataract/101_5139882.jpg</td>\n",
       "      <td>cataract</td>\n",
       "      <td>101_cataract</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>images/cataract/101_709333.jpg</td>\n",
       "      <td>cataract</td>\n",
       "      <td>101_cataract</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4011</th>\n",
       "      <td>images/normal/8_right.jpg</td>\n",
       "      <td>normal</td>\n",
       "      <td>8_normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4012</th>\n",
       "      <td>images/normal/939_left.jpg</td>\n",
       "      <td>normal</td>\n",
       "      <td>939_normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4013</th>\n",
       "      <td>images/normal/939_right.jpg</td>\n",
       "      <td>normal</td>\n",
       "      <td>939_normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4014</th>\n",
       "      <td>images/normal/951_left.jpg</td>\n",
       "      <td>normal</td>\n",
       "      <td>951_normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4015</th>\n",
       "      <td>images/normal/951_right.jpg</td>\n",
       "      <td>normal</td>\n",
       "      <td>951_normal</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4016 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       image_filepath     label    patient_id\n",
       "0       images/cataract/0_4015166.jpg  cataract    0_cataract\n",
       "1          images/cataract/0_left.jpg  cataract    0_cataract\n",
       "2      images/cataract/100_334408.jpg  cataract  100_cataract\n",
       "3     images/cataract/101_5139882.jpg  cataract  101_cataract\n",
       "4      images/cataract/101_709333.jpg  cataract  101_cataract\n",
       "...                               ...       ...           ...\n",
       "4011        images/normal/8_right.jpg    normal      8_normal\n",
       "4012       images/normal/939_left.jpg    normal    939_normal\n",
       "4013      images/normal/939_right.jpg    normal    939_normal\n",
       "4014       images/normal/951_left.jpg    normal    951_normal\n",
       "4015      images/normal/951_right.jpg    normal    951_normal\n",
       "\n",
       "[4016 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use filenames and folders to get filepath and labels for all  images\n",
    "all_imagefiles_list = glob.glob(\n",
    "    'images/**/*.jpg', \n",
    "    recursive=True\n",
    ")  # resized to 456x456 .jpg earlier\n",
    "\n",
    "labels_temp = [filepath.split('/')[-2] for filepath in all_imagefiles_list]\n",
    "patient_id = [f\"{Path(filepath).stem.split('_')[0]}_{Path(filepath).parent.name}\" for filepath in all_imagefiles_list]\n",
    "\n",
    "# create dataframe with image_filepath, label\n",
    "full_data_df = pd.DataFrame({\n",
    "    'image_filepath': all_imagefiles_list,\n",
    "    'label': labels_temp,\n",
    "    'patient_id': patient_id\n",
    "})\n",
    "\n",
    "full_data_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b02d75be-c5e8-4a65-93c8-cdcd707827bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the full data into train, validation, and test sets based on patient_id. \n",
    "# Use 80% for training, 10% for validation, and 10% for testing.\n",
    "# Split by patient_id ensures that images from the same patient do not appear in both training and validation sets or\n",
    "# training and test sets, or validation and tests sets.\n",
    "# As eye disease may affect both eyes, this is important to prevent data leakage.\n",
    "\n",
    "train_ids, not_train_ids = train_test_split(\n",
    "    full_data_df['patient_id'].unique(), \n",
    "    test_size=0.2, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "val_ids, test_ids = train_test_split(\n",
    "    not_train_ids, \n",
    "    test_size=0.5, \n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f22fb306-7fb4-48d5-9e74-8e30691e4f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataframes for train, validation, and test sets\n",
    "\n",
    "train_df = full_data_df[full_data_df['patient_id'].isin(train_ids)].reset_index(drop=True)\n",
    "val_df = full_data_df[full_data_df['patient_id'].isin(val_ids)].reset_index(drop=True)\n",
    "test_df = full_data_df[full_data_df['patient_id'].isin(test_ids)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cdf53f25-e701-44d9-9179-7843bb177393",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "diabetic_retinopathy    894\n",
      "normal                  866\n",
      "cataract                730\n",
      "glaucoma                730\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# how many images of each label (in training data)?\n",
    "print(train_df[\"label\"].value_counts())\n",
    "comment = \"\"\"\n",
    "Classes appear roughly balanced in quantity.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "453e19e5-382d-4c65-8d42-db15bae5d7e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "normal                  126\n",
      "diabetic_retinopathy    100\n",
      "cataract                 88\n",
      "glaucoma                 80\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# how many images of each label (in validation data)?\n",
    "print(val_df[\"label\"].value_counts())\n",
    "comment = \"\"\"\n",
    "Classes are reasonably balanced in quantity .\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c2390949-cdbd-4362-a14e-bd1cc9fb5ebd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "cataract                120\n",
      "diabetic_retinopathy    104\n",
      "glaucoma                 96\n",
      "normal                   82\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# how many images of each label (in test data)?\n",
    "print(test_df[\"label\"].value_counts())\n",
    "comment = \"\"\"\n",
    "Classes are reasonablybalanced in quantity .\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "09602f6a-9205-41aa-a0b1-d02792ccfb2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create integer class labels\n",
    "# Instead of one-hot booleans, we’ll use integer class indices (better for CrossEntropyLoss).\n",
    "\n",
    "full_data_labels = full_data_df['label'].values.tolist()\n",
    "unique_labels = sorted(np.unique(full_data_labels))  # e.g. ['cataract', 'diabetic_retinopathy', 'glaucoma', 'normal']\n",
    "\n",
    "label_to_idx = {label: idx for idx, label in enumerate(unique_labels)}\n",
    "idx_to_label = {idx: label for label, idx in label_to_idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dd666431-86fb-415d-8503-b967d0de255f",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = len(unique_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7c8cb33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train and y_train \n",
    "train_labels = train_df['label'].values.tolist()\n",
    "\n",
    "int_labels = [label_to_idx[label] for label in train_labels]\n",
    "\n",
    "X_train = train_df[\"image_filepath\"].tolist()\n",
    "y_train = int_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "42035379",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_val and y_val \n",
    "val_labels = val_df['label'].values.tolist()\n",
    "\n",
    "int_labels = [label_to_idx[label] for label in val_labels]\n",
    "\n",
    "X_val = val_df[\"image_filepath\"].tolist()\n",
    "y_val = int_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "80e2c081",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_test and y_test \n",
    "test_labels = test_df['label'].values.tolist()\n",
    "\n",
    "int_labels = [label_to_idx[label] for label in test_labels]\n",
    "\n",
    "X_test = test_df[\"image_filepath\"].tolist()\n",
    "y_test = int_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a99d6d2d-a622-46d7-b49c-d8e1873cf1e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3220, 3220, 394, 394, 402, 402)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check lengths of split data\n",
    "len(X_train), len(y_train), len(X_val), len(y_val), len(X_test), len(y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b1baebfb-0b14-44a7-b0c1-ed43bc1cb2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameter: image size (assuming square image size)\n",
    "\n",
    "# parameter: image size \n",
    "IMG_SIZE_HEIGHT = 456 # recommended image size for EfficientNetB5\n",
    "IMG_SIZE_WIDTH = 456\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "56586e58-8c62-49dd-b209-87accc1d77e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn data into Tensors\n",
    "# labels are already in numeric format\n",
    "# so we need to process the images\n",
    "\n",
    "# preprocessing images: turning images into tensors\n",
    "# create helpful functions for pre-processing\n",
    "\n",
    "comment = '''\n",
    "\n",
    "1. take an image filepath as input\n",
    "2. use TF to read the file and savite it to a variable, \"image\"\n",
    "3. turn our \"image\" into a tensors\n",
    "4. Normalize range of image channels from 0-255 to 0 - 1 (normalization)\n",
    "5. resize images to be the same dimensions/size, namely shape (224, 224) for this project\n",
    "6. return the modified image\n",
    "\n",
    "helpful resources: \n",
    "https://www.tensorflow.org/guide/data\n",
    "https://www.tensorflow.org/tutorials/load_data/images\n",
    "'''\n",
    "\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "def process_image(image_path, img_height=IMG_SIZE_HEIGHT, img_width=IMG_SIZE_WIDTH):\n",
    "    \"\"\"\n",
    "    Reads image from disk -> decodes -> pads to square -> converts to float32 [0,1] -> resizes.\n",
    "    \"\"\"\n",
    "    image = tf.io.read_file(image_path)\n",
    "\n",
    "    # decode_image works for jpg/png; sets shape dynamically\n",
    "    image = tf.image.decode_image(image, channels=3, expand_animations=False)\n",
    "    image = tf.ensure_shape(image, [None, None, 3])\n",
    "\n",
    "    # pad to square\n",
    "    h = tf.shape(image)[0]\n",
    "    w = tf.shape(image)[1]\n",
    "    side = tf.maximum(h, w)\n",
    "    image = tf.image.pad_to_bounding_box(\n",
    "        image,\n",
    "        (side - h) // 2,\n",
    "        (side - w) // 2,\n",
    "        side,\n",
    "        side\n",
    "    )\n",
    "\n",
    "    # normalize to [0,1]\n",
    "    image = tf.image.convert_image_dtype(image, tf.float32)\n",
    "\n",
    "    # resize\n",
    "    image = tf.image.resize(image, [img_height, img_width], method=tf.image.ResizeMethod.AREA)\n",
    "    return image\n",
    "\n",
    "\n",
    "def make_dataset(image_paths, labels=None, batch_size=32, shuffle=False):\n",
    "    \"\"\"\n",
    "    Creates a tf.data.Dataset yielding (image, label) if labels provided else just image.\n",
    "    IMPORTANT: shuffle=False preserves order so predictions align with image_paths.\n",
    "    \"\"\"\n",
    "    if labels is None:\n",
    "        ds = tf.data.Dataset.from_tensor_slices(tf.constant(image_paths))\n",
    "        if shuffle:\n",
    "            ds = ds.shuffle(buffer_size=len(image_paths), reshuffle_each_iteration=True)\n",
    "        ds = ds.map(lambda p: process_image(p), num_parallel_calls=AUTOTUNE)\n",
    "    else:\n",
    "        ds = tf.data.Dataset.from_tensor_slices((tf.constant(image_paths), tf.constant(labels)))\n",
    "        if shuffle:\n",
    "            ds = ds.shuffle(buffer_size=len(image_paths), reshuffle_each_iteration=True)\n",
    "        ds = ds.map(lambda p, y: (process_image(p), y), num_parallel_calls=AUTOTUNE)\n",
    "\n",
    "    ds = ds.batch(batch_size).prefetch(AUTOTUNE)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "94ac258e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One function to build Results DataFrames (no unbatching)\n",
    "\n",
    "def build_results_df(model, image_paths, y_int, idx_to_label, batch_size=32):\n",
    "    \"\"\"\n",
    "    Returns a DataFrame with True_Label, Prediction_Label, Image_Path.\n",
    "    Assumes y_int are integer class indices (Sparse labels).\n",
    "    \"\"\"\n",
    "    # dataset MUST be shuffle=False to preserve alignment with image_paths\n",
    "    ds = make_dataset(image_paths, labels=y_int, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    pred_probs = model.predict(ds, verbose=1)\n",
    "    pred_idx = np.argmax(pred_probs, axis=1)\n",
    "\n",
    "    true_labels = [idx_to_label[int(i)] for i in y_int]\n",
    "    pred_labels = [idx_to_label[int(i)] for i in pred_idx]\n",
    "\n",
    "    return pd.DataFrame({\n",
    "        \"True_Label\": true_labels,\n",
    "        \"Prediction_Label\": pred_labels,\n",
    "        \"Image_Path\": list(image_paths),\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d00dcd1b-9b69-4cef-8da4-8e8ffaf8f1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# turining our data into batches\n",
    "comment = '''\n",
    "Why turn our data into batches?\n",
    "Speeds up processing compared to trying to process all 10,000+ images in one go. All those images will probably not fit into RAM or VRAM (GPU)\n",
    "Batch size of 32 is recommended. So process 32 images at a time\n",
    "\n",
    "To use TF, data needs to be in the form of a Tensor tuples which look like:\n",
    "(image, label)\n",
    "'''\n",
    "\n",
    "# function to return a tuple (image, label)\n",
    "\n",
    "def get_image_label(image_path, label, img_height=IMG_SIZE_HEIGHT, img_width=IMG_SIZE_WIDTH):\n",
    "    '''\n",
    "    Takes image file path name and its label,\n",
    "    processes the images and retunrs a tuple of (image, label)\n",
    "    '''\n",
    "    image = process_image(image_path, img_height=img_height, img_width=img_width)\n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7eb93275-61cb-439c-ab39-cf6867d536c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn all our data, X and y, into batches of size 32\n",
    "comment = '''\n",
    "\n",
    "'''\n",
    "\n",
    "def create_data_batches(X, y=None, batch_size=BATCH_SIZE, shuffle=False):\n",
    "    '''\n",
    "    Creates batches of data out of image X and label y pairs\n",
    "    shuffles the data if its training data (precautionary measure)\n",
    "    doesn't shuffle if it's validation data\n",
    "    Also accepts test data as input (no labels)\n",
    "    '''\n",
    "\n",
    "    if shuffle == False:\n",
    "        # do not shuffle data\n",
    "        if y is None:\n",
    "            print('Creating data batches, no y labels provided...') # test data\n",
    "            data = tf.data.Dataset.from_tensor_slices( tf.constant(X) ) # only filepaths, no labels\n",
    "            data_batch = data.map(process_image).batch(batch_size)\n",
    "        else: # if y labels provided\n",
    "            print('Creating data batches, y labels provided...') # validation data\n",
    "            data = tf.data.Dataset.from_tensor_slices( (tf.constant(X), tf.constant(y, dtype=tf.float32)) ) # filepaths, labels\n",
    "            data_batch = data.map(get_image_label).batch(batch_size)\n",
    "    else:\n",
    "        # do shuffle data\n",
    "        if y is None:\n",
    "            print('Creating data batches, no y labels provided. Shuffling data...') # ? no need to shuffle non-training data that has no labels provided\n",
    "            data = tf.data.Dataset.from_tensor_slices( tf.constant(X) ) # only filepaths, no labels\n",
    "            # shuffle all the data\n",
    "            data = data.shuffle(buffer_size=len(X))\n",
    "            data_batch = data.map(process_image).batch(batch_size)\n",
    "        else: # if y labels provided\n",
    "            print('Creating data batches, y labels provided. Shuffling data...') # training data\n",
    "            data = tf.data.Dataset.from_tensor_slices( (tf.constant(X), tf.constant(y, dtype=tf.float32)) ) # filepaths, labels\n",
    "            # shuffle all the data\n",
    "            data = data.shuffle(buffer_size=len(X)) \n",
    "            # create (image, label) tuples\n",
    "            data_batch = data.map(get_image_label).batch(batch_size)\n",
    "\n",
    "    return data_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ba47acc8-0758-4bd5-a361-82532563df64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating data batches, y labels provided...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1767546642.854607   43229 gpu_device.cc:2020] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5564 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
     ]
    }
   ],
   "source": [
    "# create training and validation data batches\n",
    "# shuffle training data just in case there is spurious structure in the ordering of the data that may adversely affect model training\n",
    "train_data = create_data_batches(X_train, y_train, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "88b80545-0411-45c4-a5b3-df1a7d082af9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating data batches, y labels provided...\n"
     ]
    }
   ],
   "source": [
    "# create validation data batches\n",
    "val_data = create_data_batches(X_val, y_val, batch_size=BATCH_SIZE, shuffle=False) # no need to shuffle validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "41eb6888-4884-40b5-9655-b5dcf2de6e34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating data batches, y labels provided...\n"
     ]
    }
   ],
   "source": [
    "# create test data batches\n",
    "test_data = create_data_batches(X_test, y_test, batch_size=BATCH_SIZE, shuffle=False) # no need to shuffle test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "92cb5b4b-681b-4d02-8665-c26891b218c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((TensorSpec(shape=(None, 456, 456, 3), dtype=tf.float32, name=None),\n",
       "  TensorSpec(shape=(None,), dtype=tf.float32, name=None)),\n",
       " (TensorSpec(shape=(None, 456, 456, 3), dtype=tf.float32, name=None),\n",
       "  TensorSpec(shape=(None,), dtype=tf.float32, name=None)),\n",
       " (TensorSpec(shape=(None, 456, 456, 3), dtype=tf.float32, name=None),\n",
       "  TensorSpec(shape=(None,), dtype=tf.float32, name=None)))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check attriubutes of our data batches\n",
    "train_data.element_spec, val_data.element_spec, test_data.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7b6cb92d-eb51-4639-94a1-80f061fbb299",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick a model from TensorFlow Hub that suits our problem (transfer learning)\n",
    "comments = '''\n",
    "Before we build a model, there are a few things we need to define\n",
    "1. the input shape (in the form of tensors) to our model\n",
    "2. the output shape (in the form of tensors) of our model\n",
    "3. the URL of the model we want to use\n",
    "'''\n",
    "\n",
    "# setup input shape to the model\n",
    "INPUT_SHAPE = [None, IMG_SIZE_HEIGHT, IMG_SIZE_WIDTH, 3] #[Batch, Height, Width, Color_Channels]\n",
    "#INPUT_SHAPE = [IMG_SIZE_HEIGHT, IMG_SIZE_WIDTH, 3] #[Batch, Height, Width, Color_Channels]\n",
    "\n",
    "\n",
    "# output shape\n",
    "OUTPUT_SHAPE = len(unique_labels) # an array of unique classification labels\n",
    "\n",
    "# Setup model URL from TensorFlow Hub. See: https://www.kaggle.com/models/tensorflow/efficientnet/TensorFlow2/b5-feature-vector/1\n",
    "MODEL_URL = 'https://www.kaggle.com/models/tensorflow/efficientnet/TensorFlow2/b5-feature-vector/1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "59d0065d-64f5-46c6-802d-de3c4d9bed6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put it all together into a keras deep-learning model!\n",
    "# use the TF keras API. Keras is a user friendly high level API for building tensorflow models\n",
    "comment = '''\n",
    "create function that takes input of \n",
    "input shape\n",
    "output shape\n",
    "model\n",
    "\n",
    "Function defines layers in sequential fashion, compiles model, builds model, returns the model\n",
    "\n",
    "see: https://www.tensorflow.org/guide/keras/overview\n",
    "'''\n",
    "\n",
    "def create_model( input_shape = INPUT_SHAPE, output_shape = OUTPUT_SHAPE, model_url = MODEL_URL):\n",
    "    print(\"Building model with: \".format(model_url))\n",
    "\n",
    "    # setup model layers\n",
    "    model = tf.keras.Sequential([\n",
    "        hub.KerasLayer(model_url),\n",
    "        tf.keras.layers.Dense(output_shape, activation=\"softmax\"),\n",
    "    ])\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(),\n",
    "        loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "\n",
    "    # build model\n",
    "    model.build(input_shape)\n",
    "\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "da63fae8-d0f6-4191-8f39-8ea0094049e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model with: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/helsharif/miniconda3/envs/env003/lib/python3.11/site-packages/keras/src/export/tf2onnx_lib.py:8: FutureWarning: In the future `np.object` will be defined as the corresponding NumPy scalar.\n",
      "  if not hasattr(np, \"object\"):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " keras_layer (KerasLayer)    (None, 2048)              28513520  \n",
      "                                                                 \n",
      " dense (Dense)               (None, 4)                 8196      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 28521716 (108.80 MB)\n",
      "Trainable params: 8196 (32.02 KB)\n",
      "Non-trainable params: 28513520 (108.77 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Create model and display summary\n",
    "model = create_model( input_shape = INPUT_SHAPE, output_shape = OUTPUT_SHAPE, model_url = MODEL_URL )\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "33460d09-a9a3-493a-977f-8ba18d426e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create callbacks: helper functions a model can use during training to save progress, check progress, or stop training early if model does not improve\n",
    "# see: https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/TensorBoard\n",
    "comment = '''\n",
    "To start a TensorBoard session from VSC:\n",
    "\n",
    "1. Open the command palette (Ctrl/Cmd + Shift + P)\n",
    "2. Search for the command “Python: Launch TensorBoard” and press enter.\n",
    "3. You will be able to select the folder where your TensorBoard log files are located. \n",
    "   By default, the current working directory will be used.\n",
    "'''\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "703c1d8a-166c-4608-86ca-a0f35d0e820f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create function to build a tensorboard callback\n",
    "def create_tensorboard_callback():\n",
    "    logdir = os.path.join(\"logs/\",datetime.datetime.now().strftime('%Y%m%d-%H%M%S'))\n",
    "    return tf.keras.callbacks.TensorBoard(log_dir=logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "68c46e11-58b8-4668-9ebc-3fbe6357e451",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create function to stop training if it is not improving significantly anymore. Early stopping. See: https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/EarlyStopping\n",
    "\n",
    "early_stopping_callback = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "328e2d86-2c08-43a1-a62f-9b30b87d76f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model checkpoint\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath= os.path.join(\"saved_models_tensorflow/\",datetime.datetime.now().strftime('%Y%m%d-%H%M%S'),'best_model.keras'), \n",
    "    save_weights_only=False, # Set to True if you only want to save weights\n",
    "    monitor='val_accuracy', # Metric to monitor (e.g., 'val_accuracy', 'val_loss')\n",
    "    mode='max', # 'min' for loss, 'max' for accuracy\n",
    "    save_best_only=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8e1a56c7-f2b1-434c-bbbb-062154b2efcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training a model on a subset of data (only 1000 images to make sure program works before training on 10,000+ images)\n",
    "#NUM_EPOCHS = 10\n",
    "NUM_EPOCHS = 50\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "317d621b-568f-4649-ae5d-6b024fc26504",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and return a trained model\n",
    "\n",
    "def train_model():\n",
    "    '''\n",
    "    Trains a model and returns the trained version\n",
    "    '''\n",
    "    # create model\n",
    "    model = create_model()\n",
    "\n",
    "    # create new tensorboard session everytime we train a model\n",
    "    tensorboard_callback = create_tensorboard_callback()\n",
    "\n",
    "    # fit model, also passing it the callbacks\n",
    "    model.fit(\n",
    "        x=train_data, \n",
    "        epochs=NUM_EPOCHS,\n",
    "        validation_data = val_data,\n",
    "        validation_freq = 1,\n",
    "        callbacks = [tensorboard_callback, early_stopping_callback, model_checkpoint_callback],\n",
    "    )\n",
    "\n",
    "    # return fitted model\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "af99bdad-4924-4b89-9856-ed23ffa51a39",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model with: \n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-04 12:11:14.172507: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:473] Loaded cuDNN version 91500\n",
      "2026-01-04 12:11:15.311781: I external/local_xla/xla/service/service.cc:163] XLA service 0x7a8e484cb150 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2026-01-04 12:11:15.311811: I external/local_xla/xla/service/service.cc:171]   StreamExecutor device (0): NVIDIA GeForce RTX 4060 Laptop GPU, Compute Capability 8.9\n",
      "2026-01-04 12:11:15.317295: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "I0000 00:00:1767546675.409340   43312 device_compiler.h:196] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101/101 [==============================] - 101s 866ms/step - loss: 1.6859 - accuracy: 0.6519 - val_loss: 4.4336 - val_accuracy: 0.3198\n",
      "Epoch 2/50\n",
      "101/101 [==============================] - 142s 1s/step - loss: 1.7401 - accuracy: 0.5497 - val_loss: 2.3679 - val_accuracy: 0.4492\n",
      "Epoch 3/50\n",
      "101/101 [==============================] - 91s 899ms/step - loss: 1.2387 - accuracy: 0.6627 - val_loss: 1.9277 - val_accuracy: 0.5000\n",
      "Epoch 4/50\n",
      "  3/101 [..............................] - ETA: 3:22 - loss: 1.8393 - accuracy: 0.6250"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[34]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Fit model to data\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m model = \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[33]\u001b[39m\u001b[32m, line 14\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     11\u001b[39m tensorboard_callback = create_tensorboard_callback()\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# fit model, also passing it the callbacks\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mNUM_EPOCHS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalidation_freq\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mtensorboard_callback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mearly_stopping_callback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_checkpoint_callback\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[38;5;66;03m# return fitted model\u001b[39;00m\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/env003/lib/python3.11/site-packages/tf_keras/src/utils/traceback_utils.py:65\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     63\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     64\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m65\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     66\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     67\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/env003/lib/python3.11/site-packages/tf_keras/src/engine/training.py:1804\u001b[39m, in \u001b[36mModel.fit\u001b[39m\u001b[34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[39m\n\u001b[32m   1796\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m tf.profiler.experimental.Trace(\n\u001b[32m   1797\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1798\u001b[39m     epoch_num=epoch,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1801\u001b[39m     _r=\u001b[32m1\u001b[39m,\n\u001b[32m   1802\u001b[39m ):\n\u001b[32m   1803\u001b[39m     callbacks.on_train_batch_begin(step)\n\u001b[32m-> \u001b[39m\u001b[32m1804\u001b[39m     tmp_logs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1805\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m data_handler.should_sync:\n\u001b[32m   1806\u001b[39m         context.async_wait()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/env003/lib/python3.11/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    148\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m150\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    152\u001b[39m   filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/env003/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:833\u001b[39m, in \u001b[36mFunction.__call__\u001b[39m\u001b[34m(self, *args, **kwds)\u001b[39m\n\u001b[32m    830\u001b[39m compiler = \u001b[33m\"\u001b[39m\u001b[33mxla\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mnonXla\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    832\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m._jit_compile):\n\u001b[32m--> \u001b[39m\u001b[32m833\u001b[39m   result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    835\u001b[39m new_tracing_count = \u001b[38;5;28mself\u001b[39m.experimental_get_tracing_count()\n\u001b[32m    836\u001b[39m without_tracing = (tracing_count == new_tracing_count)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/env003/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:869\u001b[39m, in \u001b[36mFunction._call\u001b[39m\u001b[34m(self, *args, **kwds)\u001b[39m\n\u001b[32m    866\u001b[39m   \u001b[38;5;28mself\u001b[39m._lock.release()\n\u001b[32m    867\u001b[39m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[32m    868\u001b[39m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m869\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtracing_compilation\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    870\u001b[39m \u001b[43m      \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_no_variable_creation_config\u001b[49m\n\u001b[32m    871\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    872\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._variable_creation_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    873\u001b[39m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[32m    874\u001b[39m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[32m    875\u001b[39m   \u001b[38;5;28mself\u001b[39m._lock.release()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/env003/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001b[39m, in \u001b[36mcall_function\u001b[39m\u001b[34m(args, kwargs, tracing_options)\u001b[39m\n\u001b[32m    137\u001b[39m bound_args = function.function_type.bind(*args, **kwargs)\n\u001b[32m    138\u001b[39m flat_inputs = function.function_type.unpack_inputs(bound_args)\n\u001b[32m--> \u001b[39m\u001b[32m139\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[32m    140\u001b[39m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[32m    141\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/env003/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1322\u001b[39m, in \u001b[36mConcreteFunction._call_flat\u001b[39m\u001b[34m(self, tensor_inputs, captured_inputs)\u001b[39m\n\u001b[32m   1318\u001b[39m possible_gradient_type = gradients_util.PossibleTapeGradientTypes(args)\n\u001b[32m   1319\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type == gradients_util.POSSIBLE_GRADIENT_TYPES_NONE\n\u001b[32m   1320\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[32m   1321\u001b[39m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1322\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_inference_function\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1323\u001b[39m forward_backward = \u001b[38;5;28mself\u001b[39m._select_forward_and_backward_functions(\n\u001b[32m   1324\u001b[39m     args,\n\u001b[32m   1325\u001b[39m     possible_gradient_type,\n\u001b[32m   1326\u001b[39m     executing_eagerly)\n\u001b[32m   1327\u001b[39m forward_function, args_with_tangents = forward_backward.forward()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/env003/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:216\u001b[39m, in \u001b[36mAtomicFunction.call_preflattened\u001b[39m\u001b[34m(self, args)\u001b[39m\n\u001b[32m    214\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core.Tensor]) -> Any:\n\u001b[32m    215\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m   flat_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    217\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.function_type.pack_output(flat_outputs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/env003/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:251\u001b[39m, in \u001b[36mAtomicFunction.call_flat\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m record.stop_recording():\n\u001b[32m    250\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._bound_context.executing_eagerly():\n\u001b[32m--> \u001b[39m\u001b[32m251\u001b[39m     outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_bound_context\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunction_type\u001b[49m\u001b[43m.\u001b[49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    256\u001b[39m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    257\u001b[39m     outputs = make_call_op_in_graph(\n\u001b[32m    258\u001b[39m         \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    259\u001b[39m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[32m    260\u001b[39m         \u001b[38;5;28mself\u001b[39m._bound_context.function_call_options.as_attrs(),\n\u001b[32m    261\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/env003/lib/python3.11/site-packages/tensorflow/python/eager/context.py:1688\u001b[39m, in \u001b[36mContext.call_function\u001b[39m\u001b[34m(self, name, tensor_inputs, num_outputs)\u001b[39m\n\u001b[32m   1686\u001b[39m cancellation_context = cancellation.context()\n\u001b[32m   1687\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1688\u001b[39m   outputs = \u001b[43mexecute\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1689\u001b[39m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1690\u001b[39m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1691\u001b[39m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1692\u001b[39m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1693\u001b[39m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1694\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1695\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1696\u001b[39m   outputs = execute.execute_with_cancellation(\n\u001b[32m   1697\u001b[39m       name.decode(\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m   1698\u001b[39m       num_outputs=num_outputs,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1702\u001b[39m       cancellation_manager=cancellation_context,\n\u001b[32m   1703\u001b[39m   )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/env003/lib/python3.11/site-packages/tensorflow/python/eager/execute.py:53\u001b[39m, in \u001b[36mquick_execute\u001b[39m\u001b[34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[39m\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     52\u001b[39m   ctx.ensure_initialized()\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m   tensors = \u001b[43mpywrap_tfe\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m core._NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     56\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Fit model to data\n",
    "model = train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6683ef12-719b-47a1-9fab-0a986d0d92af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Best Model\n",
    "search_pattern = 'saved_models_tensorflow/**/best_model.keras'\n",
    "list_of_files = glob.glob(search_pattern, recursive=True)\n",
    "\n",
    "if not list_of_files:\n",
    "    print(\"No file named 'best_model.keras' found in 'saved_models_tensorflow' subfolders.\")\n",
    "else:\n",
    "    # Find the file with the maximum creation time (os.path.getctime)\n",
    "    # or modification time (os.path.getmtime)\n",
    "    latest_file = max(list_of_files, key=os.path.getctime)\n",
    "    print(f\"The latest created file is: {latest_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e7dc227-f514-41dc-83cc-ff51641bc89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try loading the best model\n",
    "# note, need to add custom_objects={\"KerasLayer\":hub.KerasLayer} because of using TensorHub\n",
    "loaded_best_model = tf.keras.models.load_model(latest_file, custom_objects={\"KerasLayer\":hub.KerasLayer})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab22129",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get prediction probabilities, don't unbatch\n",
    "train_probs = loaded_best_model.predict(train_data, verbose=1)\n",
    "val_probs   = loaded_best_model.predict(val_data, verbose=1)\n",
    "test_probs  = loaded_best_model.predict(test_data, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8db5d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert probabilities to predicted class indices\n",
    "train_preds = np.argmax(train_probs, axis=1)\n",
    "val_preds   = np.argmax(val_probs, axis=1)\n",
    "test_preds  = np.argmax(test_probs, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acddf553",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract true labels (clean + aligned with predictions), still unbatched\n",
    "\n",
    "def get_true_labels(dataset):\n",
    "    return np.concatenate([y.numpy() for _, y in dataset], axis=0)\n",
    "\n",
    "y_train = get_true_labels(train_data)\n",
    "y_val   = get_true_labels(val_data)\n",
    "y_test  = get_true_labels(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a74649",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(\"Train acc:\", accuracy_score(y_train, train_preds))\n",
    "print(\"Val acc:  \", accuracy_score(y_val, val_preds))\n",
    "print(\"Test acc: \", accuracy_score(y_test, test_preds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09fc78a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cm = confusion_matrix(y_test, test_preds)\n",
    "disp = ConfusionMatrixDisplay(cm, display_labels=CLASS_NAMES)\n",
    "\n",
    "disp.plot(cmap=\"Blues\", xticks_rotation=45)\n",
    "plt.title(\"Test Confusion Matrix\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2032f791",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(\n",
    "    classification_report(\n",
    "        y_test,\n",
    "        test_preds,\n",
    "        target_names=unique_labels,\n",
    "        digits=4,\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7292119d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1074122a-40c2-468f-ade0-7ae333f29a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View TensorBoard\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a1dbd0-b6f4-46dc-b615-f8c92d2d3c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation metrics on Train, Validation, and Test data\n",
    "metric_names = [\"Loss\", \"Accuracy\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e12655f-e7fd-40a2-b6b0-33828212bd55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# Evaluate Train Metrics\n",
    "# -----------------------\n",
    "train_metrics = loaded_best_model.evaluate(train_data, verbose=0);\n",
    "\n",
    "train_metrics_df = pd.DataFrame({\n",
    "    \"Metric_Name\": metric_names,\n",
    "    \"Value\": train_metrics,\n",
    "})\n",
    "\n",
    "train_metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893d84cb-6ad4-4af4-83e2-65f9580158eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# Evaluate Validation Metrics\n",
    "# -----------------------\n",
    "\n",
    "val_metrics = loaded_best_model.evaluate(val_data, verbose=0);\n",
    "\n",
    "val_metrics_df = pd.DataFrame({\n",
    "    \"Metric_Name\": metric_names,\n",
    "    \"Value\": val_metrics,\n",
    "})\n",
    "\n",
    "val_metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77641de-0fa8-4fcc-894e-4a2558a96edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# Evaluate Test Metrics\n",
    "# -----------------------\n",
    "\n",
    "test_metrics = loaded_best_model.evaluate(test_data, verbose=0);\n",
    "\n",
    "test_metrics_df = pd.DataFrame({\n",
    "    \"Metric_Name\": metric_names,\n",
    "    \"Value\": test_metrics,\n",
    "})\n",
    "\n",
    "test_metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb978be-87ab-4fd1-b6af-296cff274506",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now predict at scale with visualization of images\n",
    "\n",
    "# turn probabilities into their respective label (for ease of understanding)\n",
    "\n",
    "def get_pred_label(prediction_probs, class_names):\n",
    "    \"\"\"\n",
    "    Convert model prediction probabilities to class label.\n",
    "    \"\"\"\n",
    "    class_idx = int(np.argmax(prediction_probs))\n",
    "    return class_names[class_idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cfef6d7-8b8b-440f-b0d4-77d475bf5736",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions\n",
    "\n",
    "train_data_predictions = loaded_best_model.predict(train_data, verbose=1)\n",
    "val_data_predictions = loaded_best_model.predict(val_data, verbose=1)\n",
    "test_data_predictions = loaded_best_model.predict(test_data, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f62723-3d33-4e36-bd53-bd396b775c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to unbatch the train_data to get the predictions, images and true labels\n",
    "train_data_images_unbatched_list = []\n",
    "train_data_labels_unbatched_list = []\n",
    "\n",
    "# loop through unbatched data\n",
    "for image, label in train_data.unbatch().as_numpy_iterator():\n",
    "    train_data_images_unbatched_list.append(image)\n",
    "    train_data_labels_unbatched_list.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac2f405-8f26-4ec8-93a8-85009e677f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to unbatch the val_data to get the predictions, images and true labels\n",
    "val_data_images_unbatched_list = []\n",
    "val_data_labels_unbatched_list = []\n",
    "\n",
    "# loop through unbatched data\n",
    "for image, label in val_data.unbatch().as_numpy_iterator():\n",
    "    val_data_images_unbatched_list.append(image)\n",
    "    val_data_labels_unbatched_list.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3348a83c-c1e6-45a2-b9d9-42b338af9a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to unbatch the test_data to get the predictions, images and true labels\n",
    "test_data_images_unbatched_list = []\n",
    "test_data_labels_unbatched_list = []\n",
    "\n",
    "# loop through unbatched data\n",
    "for image, label in test_data.unbatch().as_numpy_iterator():\n",
    "    test_data_images_unbatched_list.append(image)\n",
    "    test_data_labels_unbatched_list.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97445e80-d7b4-4671-8f99-5a14f1b181d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_unbatched_label = [get_pred_label(label) for label in train_data_labels_unbatched_list] # true label of validation data\n",
    "train_predictions_label = [get_pred_label(label) for label in train_data_predictions] # prediction on validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8bd175e-8a19-42cd-83c4-52fff7bdd130",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data_unbatched_label = [get_pred_label(label) for label in val_data_labels_unbatched_list] # true label of validation data\n",
    "val_predictions_label = [get_pred_label(label) for label in val_data_predictions] # prediction on validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fabc620-68bf-4448-9ba3-e415912506fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_unbatched_label = [get_pred_label(label) for label in test_data_labels_unbatched_list] # true label of validation data\n",
    "test_predictions_label = [get_pred_label(label) for label in test_data_predictions] # prediction on validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ff9dec-ee37-437b-a6fd-6883c250d7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataframes\n",
    "\n",
    "Train_Results_df = pd.DataFrame(\n",
    "    {\n",
    "        'True_Label': train_data_unbatched_label,\n",
    "        'Prediction_Label': train_predictions_label,\n",
    "        'Image_Data': train_data_images_unbatched_list,\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "Validation_Results_df = pd.DataFrame(\n",
    "    {\n",
    "        'True_Label': val_data_unbatched_label,\n",
    "        'Prediction_Label': val_predictions_label,\n",
    "        'Image_Data': val_data_images_unbatched_list,\n",
    "    }\n",
    ")\n",
    "\n",
    "Test_Results_df = pd.DataFrame(\n",
    "    {\n",
    "        'True_Label': test_data_unbatched_label,\n",
    "        'Prediction_Label': test_predictions_label,\n",
    "        'Image_Data': test_data_images_unbatched_list,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8146b3fb-2ca1-40c7-8084-1c806514ea7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show some rows of dataframe\n",
    "Train_Results_df[['True_Label', 'Prediction_Label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7a281f-6dee-4ffc-aa63-b49bcb75ca3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show some rows of dataframe\n",
    "Validation_Results_df[['True_Label', 'Prediction_Label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f81330a-8383-4c7b-86e8-9cbf770b59d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show some rows of dataframe\n",
    "Test_Results_df[['True_Label', 'Prediction_Label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e344f4eb-3a75-45f1-9fb9-b4ed593da9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Confusion Matrix\n",
    "\n",
    "train_cm = confusion_matrix(Train_Results_df['True_Label'], Train_Results_df['Prediction_Label'])\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=train_cm, display_labels=unique_labels)\n",
    "\n",
    "disp.plot(xticks_rotation=60)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5265ce77-e859-40e8-be09-e352165d4452",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation Confusion Matrix\n",
    "\n",
    "val_cm = confusion_matrix(Validation_Results_df['True_Label'], Validation_Results_df['Prediction_Label'])\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=val_cm, display_labels=unique_labels)\n",
    "\n",
    "disp.plot(xticks_rotation=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07975c8-115f-4f8b-b1d7-277a1f6b7cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Confusion Matrix\n",
    "\n",
    "test_cm = confusion_matrix(Test_Results_df['True_Label'], Test_Results_df['Prediction_Label'])\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=test_cm, display_labels=unique_labels)\n",
    "\n",
    "disp.plot(xticks_rotation=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c315e89-2bc0-4d40-a9d1-f723cec17457",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Classification Report\n",
    "\n",
    "train_report = classification_report(Train_Results_df['True_Label'], Train_Results_df['Prediction_Label'])\n",
    "print(train_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8747d5c5-73b0-4daa-b29d-b7e364a3df31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation Classification Report\n",
    "\n",
    "val_report = classification_report(Validation_Results_df['True_Label'], Validation_Results_df['Prediction_Label'])\n",
    "print(val_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad9fbd9-25c8-4547-ae7e-07a9419bf39b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Classification Report\n",
    "\n",
    "test_report = classification_report(Test_Results_df['True_Label'], Test_Results_df['Prediction_Label'])\n",
    "print(test_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead18180-f3a5-4a53-a2e6-de2c48d7980d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Figures Test Results (Plotly)\n",
    "\n",
    "# num_rows = np.ceil(np.sqrt(len(Test_Results_df))).astype(int)\n",
    "# num_cols = num_rows\n",
    "\n",
    "# df = Test_Results_df.head(100)  # shorthand, first 100 rows\n",
    "# n = len(df)\n",
    "# grid = int(np.ceil(np.sqrt(n)))\n",
    "\n",
    "# # Create list of subplot titles\n",
    "# titles = [\n",
    "#     f\"True Label: {row['True_Label']}<br>Predicted: {row['Prediction_Label']}\"\n",
    "#     for _, row in df.iterrows()\n",
    "# ]\n",
    "\n",
    "# # create figure\n",
    "# fig = make_subplots(rows=grid, cols=grid, subplot_titles=titles, vertical_spacing=0.02, horizontal_spacing=0.01)\n",
    "\n",
    "\n",
    "\n",
    "# def to_uint8(img):\n",
    "#     \"\"\"Ensure image is uint8 RGB for Plotly.\"\"\"\n",
    "#     arr = np.asarray(img)\n",
    "#     if arr.dtype == np.uint8:\n",
    "#         return arr\n",
    "#     # If floats in [0,1], scale up; otherwise clip to [0,255]\n",
    "#     if np.issubdtype(arr.dtype, np.floating) and arr.max() <= 1.0:\n",
    "#         arr = (np.clip(arr, 0.0, 1.0) * 255).astype(np.uint8)\n",
    "#     else:\n",
    "#         arr = np.clip(arr, 0, 255).astype(np.uint8)\n",
    "#     return arr\n",
    "\n",
    "# for idx, img in enumerate(df[\"Image_Data\"]):\n",
    "#     r = idx // grid + 1\n",
    "#     c = idx % grid + 1\n",
    "#     img_u8 = to_uint8(img)            # expects shape (H, W, 3)\n",
    "#     fig.add_trace(go.Image(z=img_u8), row=r, col=c)\n",
    "\n",
    "# # Hide ticks/axes and keep a nice square layout\n",
    "# fig.update_xaxes(visible=False)\n",
    "# fig.update_yaxes(visible=False)\n",
    "\n",
    "# # Optional: make figure size scale with grid\n",
    "# fig.update_layout(\n",
    "#     showlegend=False,\n",
    "#     #margin=dict(l=10, r=10, t=10, b=10),\n",
    "#     height=grid * 200,\n",
    "#     width=grid * 200,\n",
    "#     font=dict(size=2)\n",
    "# )\n",
    "\n",
    "# # Update the font size of all annotations (subplot titles)\n",
    "# fig.update_annotations(font_size=12) # Change 12 to your desired font size\n",
    "\n",
    "# fig.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaca7d27-06df-43dc-b375-d923b85aa145",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure using Matplotlib\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- Helper to ensure uint8 ---\n",
    "def to_uint8(img):\n",
    "    arr = np.asarray(img)\n",
    "    if arr.dtype == np.uint8:\n",
    "        return arr\n",
    "    if np.issubdtype(arr.dtype, np.floating) and arr.max() <= 1.0:\n",
    "        arr = (np.clip(arr, 0.0, 1.0) * 255).astype(np.uint8)\n",
    "    else:\n",
    "        arr = np.clip(arr, 0, 255).astype(np.uint8)\n",
    "    return arr\n",
    "\n",
    "# --- Setup Data ---\n",
    "df = Test_Results_df.head(100)\n",
    "n = len(df)\n",
    "grid = int(np.ceil(np.sqrt(n)))\n",
    "\n",
    "# --- Create Matplotlib Figure ---\n",
    "fig, axes = plt.subplots(grid, grid, figsize=(grid * 2, grid * 2))\n",
    "axes = axes.flatten()   # flatten for easy indexing\n",
    "\n",
    "for idx, (i, row) in enumerate(df.iterrows()):\n",
    "    ax = axes[idx]\n",
    "    img = to_uint8(row[\"Image_Data\"])\n",
    "    ax.imshow(img)\n",
    "    ax.set_title(\n",
    "        f\"True: {row['True_Label']}\\nPred: {row['Prediction_Label']}\",\n",
    "        fontsize=8\n",
    "    )\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "# Turn off any extra empty subplots\n",
    "for j in range(idx + 1, len(axes)):\n",
    "    axes[j].axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69cbf795-c5b9-4f0e-bd38-8da131027db1",
   "metadata": {},
   "source": [
    "# Test-Set Performance Evaluation\n",
    "\n",
    "Your model is a 4-class image classifier for **cataracts**, **diabetic retinopathy**, **glaucoma**, and **normal** retinal images.  \n",
    "Below is a structured interpretation of your test metrics.\n",
    "\n",
    "---\n",
    "\n",
    "## Overall Metrics\n",
    "- **Loss:** `0.3305`  \n",
    "- **Accuracy:** `0.8873`  \n",
    "- **Macro F1 Score:** `0.8840`\n",
    "\n",
    "**Interpretation:**  \n",
    "These are strong results for a 4-class medical image classification task. Accuracy near 0.89 is well above the 25% random baseline, and an F1 around 0.88 indicates balanced and reliable predictions across classes.\n",
    "\n",
    "---\n",
    "\n",
    "## Per-Class Precision\n",
    "\n",
    "| Class | Precision | Notes |\n",
    "|-------|-----------|-------|\n",
    "| **Cataract** | `0.9102` | Very strong; few false positives. |\n",
    "| **Diabetic Retinopathy** | `0.9727` | Excellent; highly reliable predictions. |\n",
    "| **Glaucoma** | `0.8161` | Weaker; more false positives. |\n",
    "| **Normal** | `0.8865` | Very good performance. |\n",
    "\n",
    "**Summary:**  \n",
    "Precision is strong overall, especially for diabetic retinopathy. Glaucoma is the only class showing moderate precision.\n",
    "\n",
    "---\n",
    "\n",
    "## Per-Class Recall\n",
    "\n",
    "| Class | Recall | Notes |\n",
    "|-------|--------|-------|\n",
    "| **Cataract** | `0.8889` | High sensitivity. |\n",
    "| **Diabetic Retinopathy** | `0.9596` | Outstanding recall. |\n",
    "| **Glaucoma** | `0.8023` | Lowest recall; misses some glaucoma cases. |\n",
    "| **Normal** | `0.8286` | Good, but could be improved. |\n",
    "\n",
    "**Summary:**  \n",
    "Recall is consistently high, but glaucoma remains the most challenging class.\n",
    "\n",
    "---\n",
    "\n",
    "## High-Level Assessment\n",
    "- **Performance is robust and clinically meaningful.**\n",
    "- The model reliably identifies all classes, especially diabetic retinopathy.\n",
    "- **Glaucoma** shows the weakest performance (expected due to subtle features).\n",
    "- No sign of extreme class imbalance or overfitting based on metrics alone.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "The model demonstrates **strong overall performance** with **Accuracy ~88.7%** and **Macro F1 ~88.4%**, delivering reliable detection across all four eye-condition classes.  \n",
    "Minor weaknesses in glaucoma detection are expected and can be improved with targeted techniques.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85721564-dce8-4609-90d2-9220b58d0d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a csv file of test_df with predictions\n",
    "timestamp = datetime.datetime.now().strftime('%Y%m%d-%H%M%S')\n",
    "test_results_output_df = test_df.copy()\n",
    "test_results_output_df['Predicted_Label'] = Test_Results_df['Prediction_Label'].values.tolist()\n",
    "output_csv_path = f\"test_results_with_predictions_tensorflow_{timestamp}.csv\"\n",
    "test_results_output_df.to_csv(output_csv_path, index=False)\n",
    "print(f\"Test results with predictions saved to {output_csv_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env003",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
