{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ac3c22-1550-4021-a8ac-ba77f53ddaaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Husayn El Sharif\n",
    "comment = \"\"\"\n",
    "Use environment: env003 which has tensorflow support\n",
    "use efficientnet from tensorhub and force 456 x 456 size\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3371f3aa-3545-4237-9881-1f1dc84fbb9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorFlow Import and GPU Check\n",
    "\n",
    "import tensorflow as tf\n",
    "import tf_keras as keras # Import tf_keras for Keras 3 compatibility\n",
    "import tensorflow_hub as hub\n",
    "print(\"TF Version:\", tf.__version__)\n",
    "print(\"TF Hub Version:\", hub.__version__)\n",
    "\n",
    "# Check if GPU is savailable\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    print(\"GPUs available:\")\n",
    "    for gpu in gpus:\n",
    "        print(f\"- {gpu}\")\n",
    "else:\n",
    "    print(\"No GPUs found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e494ad-c819-4020-8163-c9affc2e1a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# other imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "\n",
    "import datetime\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "from IPython.display import Image\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "import datetime\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import classification_report\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5b896b-1a7e-4c3b-8e7b-6311b9d30d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use filenames and folders to get filepath and labels for all  images\n",
    "all_imagefiles_list = glob.glob(\n",
    "    'images/**/*.jpg', \n",
    "    recursive=True\n",
    ")  # resized to 456x456 .jpg earlier\n",
    "\n",
    "labels_temp = [filepath.split('/')[-2] for filepath in all_imagefiles_list]\n",
    "patient_id = [f\"{Path(filepath).stem.split('_')[0]}_{Path(filepath).parent.name}\" for filepath in all_imagefiles_list]\n",
    "\n",
    "# create dataframe with image_filepath, label\n",
    "full_data_df = pd.DataFrame({\n",
    "    'image_filepath': all_imagefiles_list,\n",
    "    'label': labels_temp,\n",
    "    'patient_id': patient_id\n",
    "})\n",
    "\n",
    "full_data_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02d75be-c5e8-4a65-93c8-cdcd707827bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the full data into train, validation, and test sets based on patient_id. \n",
    "# Use 80% for training, 10% for validation, and 10% for testing.\n",
    "# Split by patient_id ensures that images from the same patient do not appear in both training and validation sets or\n",
    "# training and test sets, or validation and tests sets.\n",
    "# As eye disease may affect both eyes, this is important to prevent data leakage.\n",
    "\n",
    "train_ids, not_train_ids = train_test_split(\n",
    "    full_data_df['patient_id'].unique(), \n",
    "    test_size=0.2, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "val_ids, test_ids = train_test_split(\n",
    "    not_train_ids, \n",
    "    test_size=0.5, \n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f22fb306-7fb4-48d5-9e74-8e30691e4f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataframes for train, validation, and test sets\n",
    "\n",
    "train_df = full_data_df[full_data_df['patient_id'].isin(train_ids)].reset_index(drop=True)\n",
    "val_df = full_data_df[full_data_df['patient_id'].isin(val_ids)].reset_index(drop=True)\n",
    "test_df = full_data_df[full_data_df['patient_id'].isin(test_ids)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf53f25-e701-44d9-9179-7843bb177393",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many images of each label (in training data)?\n",
    "print(train_df[\"label\"].value_counts())\n",
    "comment = \"\"\"\n",
    "Classes appear roughly balanced in quantity.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453e19e5-382d-4c65-8d42-db15bae5d7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many images of each label (in validation data)?\n",
    "print(val_df[\"label\"].value_counts())\n",
    "comment = \"\"\"\n",
    "Classes are reasonably balanced in quantity .\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2390949-cdbd-4362-a14e-bd1cc9fb5ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many images of each label (in test data)?\n",
    "print(test_df[\"label\"].value_counts())\n",
    "comment = \"\"\"\n",
    "Classes are reasonablybalanced in quantity .\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09602f6a-9205-41aa-a0b1-d02792ccfb2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create integer class labels\n",
    "# Instead of one-hot booleans, we’ll use integer class indices (better for CrossEntropyLoss).\n",
    "\n",
    "full_data_labels = full_data_df['label'].values.tolist()\n",
    "unique_labels = sorted(np.unique(full_data_labels))  # e.g. ['cataract', 'diabetic_retinopathy', 'glaucoma', 'normal']\n",
    "\n",
    "label_to_idx = {label: idx for idx, label in enumerate(unique_labels)}\n",
    "idx_to_label = {idx: label for label, idx in label_to_idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd666431-86fb-415d-8503-b967d0de255f",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = len(unique_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8cb33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train and y_train \n",
    "train_labels = train_df['label'].values.tolist()\n",
    "\n",
    "int_labels = [label_to_idx[label] for label in train_labels]\n",
    "\n",
    "X_train = train_df[\"image_filepath\"].tolist()\n",
    "y_train = int_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42035379",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_val and y_val \n",
    "val_labels = val_df['label'].values.tolist()\n",
    "\n",
    "int_labels = [label_to_idx[label] for label in val_labels]\n",
    "\n",
    "X_val = val_df[\"image_filepath\"].tolist()\n",
    "y_val = int_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e2c081",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_test and y_test \n",
    "test_labels = test_df['label'].values.tolist()\n",
    "\n",
    "int_labels = [label_to_idx[label] for label in test_labels]\n",
    "\n",
    "X_test = test_df[\"image_filepath\"].tolist()\n",
    "y_test = int_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99d6d2d-a622-46d7-b49c-d8e1873cf1e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check lengths of split data\n",
    "len(X_train), len(y_train), len(X_val), len(y_val), len(X_test), len(y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1baebfb-0b14-44a7-b0c1-ed43bc1cb2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameter: image size (assuming square image size)\n",
    "\n",
    "# parameter: image size \n",
    "IMG_SIZE_HEIGHT = 456 # recommended image size for EfficientNetB5\n",
    "IMG_SIZE_WIDTH = 456\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56586e58-8c62-49dd-b209-87accc1d77e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn data into Tensors\n",
    "# labels are already in numeric format\n",
    "# so we need to process the images\n",
    "\n",
    "# preprocessing images: turning images into tensors\n",
    "# create helpful functions for pre-processing\n",
    "\n",
    "comment = '''\n",
    "\n",
    "1. take an image filepath as input\n",
    "2. use TF to read the file and savite it to a variable, \"image\"\n",
    "3. turn our \"image\" into a tensors\n",
    "4. Normalize range of image channels from 0-255 to 0 - 1 (normalization)\n",
    "5. resize images to be the same dimensions/size, namely shape (224, 224) for this project\n",
    "6. return the modified image\n",
    "\n",
    "helpful resources: \n",
    "https://www.tensorflow.org/guide/data\n",
    "https://www.tensorflow.org/tutorials/load_data/images\n",
    "'''\n",
    "def process_image(image_path, img_height=IMG_SIZE_HEIGHT, img_width=IMG_SIZE_WIDTH ):\n",
    "    '''\n",
    "    Takes an image file path and turns image into a Tensor\n",
    "    '''\n",
    "\n",
    "    # read image file (from TF load images documentation)\n",
    "    image = tf.io.read_file(image_path)\n",
    "    image = tf.image.decode_jpeg(image, channels=3)\n",
    "\n",
    "\n",
    "    # Ensure square via padding before resize to avoid aspect distortion\n",
    "    h = tf.shape(image)[0]\n",
    "    w = tf.shape(image)[1]\n",
    "    side = tf.maximum(h, w)\n",
    "    image = tf.image.pad_to_bounding_box(image, (side - h)//2, (side - w)//2, side, side)\n",
    "    \n",
    "\n",
    "    \n",
    "    # convert color channels from 0-255 to 0-1 values (Normalization)\n",
    "    image = tf.image.convert_image_dtype(image, tf.float32)\n",
    "    \n",
    "    # resize image\n",
    "    image = tf.image.resize(image, size=[img_height, img_width], method=tf.image.ResizeMethod.AREA)\n",
    "    # return image converted to tensor\n",
    "    return image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00dcd1b-9b69-4cef-8da4-8e8ffaf8f1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# turining our data into batches\n",
    "comment = '''\n",
    "Why turn our data into batches?\n",
    "Speeds up processing compared to trying to process all 10,000+ images in one go. All those images will probably not fit into RAM or VRAM (GPU)\n",
    "Batch size of 32 is recommended. So process 32 images at a time\n",
    "\n",
    "To use TF, data needs to be in the form of a Tensor tuples which look like:\n",
    "(image, label)\n",
    "'''\n",
    "\n",
    "# function to return a tuple (image, label)\n",
    "\n",
    "def get_image_label(image_path, label, img_height=IMG_SIZE_HEIGHT, img_width=IMG_SIZE_WIDTH):\n",
    "    '''\n",
    "    Takes image file path name and its label,\n",
    "    processes the images and retunrs a tuple of (image, label)\n",
    "    '''\n",
    "    image = process_image(image_path, img_height=img_height, img_width=img_width)\n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb93275-61cb-439c-ab39-cf6867d536c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn all our data, X and y, into batches of size 32\n",
    "comment = '''\n",
    "\n",
    "'''\n",
    "\n",
    "def create_data_batches(X, y=None, batch_size=BATCH_SIZE, shuffle=False):\n",
    "    '''\n",
    "    Creates batches of data out of image X and label y pairs\n",
    "    shuffles the data if its training data (precautionary measure)\n",
    "    doesn't shuffle if it's validation data\n",
    "    Also accepts test data as input (no labels)\n",
    "    '''\n",
    "\n",
    "    if shuffle == False:\n",
    "        # do not shuffle data\n",
    "        if y is None:\n",
    "            print('Creating data batches, no y labels provided...') # test data\n",
    "            data = tf.data.Dataset.from_tensor_slices( tf.constant(X) ) # only filepaths, no labels\n",
    "            data_batch = data.map(process_image).batch(batch_size)\n",
    "        else: # if y labels provided\n",
    "            print('Creating data batches, y labels provided...') # validation data\n",
    "            data = tf.data.Dataset.from_tensor_slices( (tf.constant(X), tf.constant(y, dtype=tf.float32)) ) # filepaths, labels\n",
    "            data_batch = data.map(get_image_label).batch(batch_size)\n",
    "    else:\n",
    "        # do shuffle data\n",
    "        if y is None:\n",
    "            print('Creating data batches, no y labels provided. Shuffling data...') # ? no need to shuffle non-training data that has no labels provided\n",
    "            data = tf.data.Dataset.from_tensor_slices( tf.constant(X) ) # only filepaths, no labels\n",
    "            # shuffle all the data\n",
    "            data = data.shuffle(buffer_size=len(X))\n",
    "            data_batch = data.map(process_image).batch(batch_size)\n",
    "        else: # if y labels provided\n",
    "            print('Creating data batches, y labels provided. Shuffling data...') # training data\n",
    "            data = tf.data.Dataset.from_tensor_slices( (tf.constant(X), tf.constant(y, dtype=tf.float32)) ) # filepaths, labels\n",
    "            # shuffle all the data\n",
    "            data = data.shuffle(buffer_size=len(X)) \n",
    "            # create (image, label) tuples\n",
    "            data_batch = data.map(get_image_label).batch(batch_size)\n",
    "\n",
    "    return data_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba47acc8-0758-4bd5-a361-82532563df64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create training and validation data batches\n",
    "# shuffle training data just in case there is spurious structure in the ordering of the data that may adversely affect model training\n",
    "train_data = create_data_batches(X_train, y_train, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b80545-0411-45c4-a5b3-df1a7d082af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create validation data batches\n",
    "val_data = create_data_batches(X_val, y_val, batch_size=BATCH_SIZE, shuffle=False) # no need to shuffle validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41eb6888-4884-40b5-9655-b5dcf2de6e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create test data batches\n",
    "test_data = create_data_batches(X_test, y_test, batch_size=BATCH_SIZE, shuffle=False) # no need to shuffle test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92cb5b4b-681b-4d02-8665-c26891b218c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check attriubutes of our data batches\n",
    "train_data.element_spec, val_data.element_spec, test_data.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6cb92d-eb51-4639-94a1-80f061fbb299",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick a model from TensorFlow Hub that suits our problem (transfer learning)\n",
    "comments = '''\n",
    "Before we build a model, there are a few things we need to define\n",
    "1. the input shape (in the form of tensors) to our model\n",
    "2. the output shape (in the form of tensors) of our model\n",
    "3. the URL of the model we want to use\n",
    "'''\n",
    "\n",
    "# setup input shape to the model\n",
    "INPUT_SHAPE = [None, IMG_SIZE_HEIGHT, IMG_SIZE_WIDTH, 3] #[Batch, Height, Width, Color_Channels]\n",
    "\n",
    "# output shape\n",
    "OUTPUT_SHAPE = len(unique_labels) # an array of unique classification labels\n",
    "\n",
    "# Setup model URL from TensorFlow Hub. See: https://www.kaggle.com/models/tensorflow/efficientnet/TensorFlow2/b5-feature-vector/1\n",
    "MODEL_URL = 'https://www.kaggle.com/models/tensorflow/efficientnet/TensorFlow2/b5-feature-vector/1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d0065d-64f5-46c6-802d-de3c4d9bed6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put it all together into a keras deep-learning model!\n",
    "# use the TF keras API. Keras is a user friendly high level API for building tensorflow models\n",
    "comment = '''\n",
    "create function that takes input of \n",
    "input shape\n",
    "output shape\n",
    "model\n",
    "\n",
    "Function defines layers in sequential fashion, compiles model, builds model, returns the model\n",
    "\n",
    "see: https://www.tensorflow.org/guide/keras/overview\n",
    "'''\n",
    "\n",
    "def create_model( input_shape = INPUT_SHAPE, output_shape = OUTPUT_SHAPE, model_url = MODEL_URL):\n",
    "    print(\"Building model with: \".format(model_url))\n",
    "\n",
    "    # setup model layers\n",
    "    model = keras.Sequential([\n",
    "            hub.KerasLayer(model_url), # Layer 1 (input layer)\n",
    "            keras.layers.Dense(units = output_shape, activation=\"softmax\"), # Layer 2 (output layer), for multi-class classficiation use softmax activation       \n",
    "        ])\n",
    "\n",
    "    # compile model\n",
    "    model.compile(\n",
    "        loss=keras.losses.SparseCategoricalCrossentropy(), # for multi-class classficiation with integer labels use SparseCategoricalCrossentropy\n",
    "        optimizer = keras.optimizers.Adam(),\n",
    "        metrics=[\n",
    "            \"accuracy\", # Fraction of correct predictions.\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # build model\n",
    "    model.build(input_shape)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da63fae8-d0f6-4191-8f39-8ea0094049e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model and display summary\n",
    "model = create_model( input_shape = INPUT_SHAPE, output_shape = OUTPUT_SHAPE, model_url = MODEL_URL )\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33460d09-a9a3-493a-977f-8ba18d426e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create callbacks: helper functions a model can use during training to save progress, check progress, or stop training early if model does not improve\n",
    "# see: https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/TensorBoard\n",
    "comment = '''\n",
    "To start a TensorBoard session from VSC:\n",
    "\n",
    "1. Open the command palette (Ctrl/Cmd + Shift + P)\n",
    "2. Search for the command “Python: Launch TensorBoard” and press enter.\n",
    "3. You will be able to select the folder where your TensorBoard log files are located. \n",
    "   By default, the current working directory will be used.\n",
    "'''\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703c1d8a-166c-4608-86ca-a0f35d0e820f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create function to build a tensorboard callback\n",
    "def create_tensorboard_callback():\n",
    "    logdir = os.path.join(\"logs/\",datetime.datetime.now().strftime('%Y%m%d-%H%M%S'))\n",
    "    return keras.callbacks.TensorBoard(log_dir=logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c46e11-58b8-4668-9ebc-3fbe6357e451",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create function to stop training if it is not improving significantly anymore. Early stopping. See: https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/EarlyStopping\n",
    "\n",
    "early_stopping_callback = keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328e2d86-2c08-43a1-a62f-9b30b87d76f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model checkpoint\n",
    "model_checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
    "    filepath= os.path.join(\"saved_models_tensorflow/\",datetime.datetime.now().strftime('%Y%m%d-%H%M%S'),'best_model.keras'), \n",
    "    save_weights_only=False, # Set to True if you only want to save weights\n",
    "    monitor='val_accuracy', # Metric to monitor (e.g., 'val_accuracy', 'val_loss')\n",
    "    mode='max', # 'min' for loss, 'max' for accuracy\n",
    "    save_best_only=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e1a56c7-f2b1-434c-bbbb-062154b2efcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training a model on a subset of data (only 1000 images to make sure program works before training on 10,000+ images)\n",
    "#NUM_EPOCHS = 10\n",
    "NUM_EPOCHS = 50\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "317d621b-568f-4649-ae5d-6b024fc26504",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and return a trained model\n",
    "\n",
    "def train_model():\n",
    "    '''\n",
    "    Trains a model and returns the trained version\n",
    "    '''\n",
    "    # create model\n",
    "    model = create_model()\n",
    "\n",
    "    # create new tensorboard session everytime we train a model\n",
    "    tensorboard_callback = create_tensorboard_callback()\n",
    "\n",
    "    # fit model, also passing it the callbacks\n",
    "    model.fit(\n",
    "        x=train_data, \n",
    "        epochs=NUM_EPOCHS,\n",
    "        validation_data = val_data,\n",
    "        validation_freq = 1,\n",
    "        callbacks = [tensorboard_callback, early_stopping_callback, model_checkpoint_callback],\n",
    "    )\n",
    "\n",
    "    # return fitted model\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af99bdad-4924-4b89-9856-ed23ffa51a39",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Fit model to data\n",
    "model = train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6683ef12-719b-47a1-9fab-0a986d0d92af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Best Model\n",
    "search_pattern = 'saved_models_tensorflow/**/best_model.keras'\n",
    "list_of_files = glob.glob(search_pattern, recursive=True)\n",
    "\n",
    "if not list_of_files:\n",
    "    print(\"No file named 'best_model.keras' found in 'saved_models_tensorflow' subfolders.\")\n",
    "else:\n",
    "    # Find the file with the maximum creation time (os.path.getctime)\n",
    "    # or modification time (os.path.getmtime)\n",
    "    latest_file = max(list_of_files, key=os.path.getctime)\n",
    "    print(f\"The latest created file is: {latest_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e7dc227-f514-41dc-83cc-ff51641bc89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try loading the best model\n",
    "# note, need to add custom_objects={\"KerasLayer\":hub.KerasLayer} because of using TensorHub\n",
    "loaded_best_model = keras.models.load_model(latest_file, custom_objects={\"KerasLayer\":hub.KerasLayer})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1074122a-40c2-468f-ade0-7ae333f29a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View TensorBoard\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a1dbd0-b6f4-46dc-b615-f8c92d2d3c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation metrics on Train, Validation, and Test data\n",
    "metric_names = [\"Loss\", \"Accuracy\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e12655f-e7fd-40a2-b6b0-33828212bd55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# Evaluate Train Metrics\n",
    "# -----------------------\n",
    "train_metrics = loaded_best_model.evaluate(train_data, verbose=0);\n",
    "\n",
    "train_metrics_df = pd.DataFrame({\n",
    "    \"Metric_Name\": metric_names,\n",
    "    \"Value\": train_metrics,\n",
    "})\n",
    "\n",
    "train_metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893d84cb-6ad4-4af4-83e2-65f9580158eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# Evaluate Validation Metrics\n",
    "# -----------------------\n",
    "\n",
    "val_metrics = loaded_best_model.evaluate(val_data, verbose=0);\n",
    "\n",
    "val_metrics_df = pd.DataFrame({\n",
    "    \"Metric_Name\": metric_names,\n",
    "    \"Value\": val_metrics,\n",
    "})\n",
    "\n",
    "val_metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77641de-0fa8-4fcc-894e-4a2558a96edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# Evaluate Test Metrics\n",
    "# -----------------------\n",
    "\n",
    "test_metrics = loaded_best_model.evaluate(test_data, verbose=0);\n",
    "\n",
    "test_metrics_df = pd.DataFrame({\n",
    "    \"Metric_Name\": metric_names,\n",
    "    \"Value\": test_metrics,\n",
    "})\n",
    "\n",
    "test_metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb978be-87ab-4fd1-b6af-296cff274506",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now predict at scale with visualization of images\n",
    "\n",
    "# turn probabilities into their respective label (for ease of understanding)\n",
    "\n",
    "def get_pred_label(prediction_probs, class_names):\n",
    "    \"\"\"\n",
    "    Convert model prediction probabilities to class label.\n",
    "    \"\"\"\n",
    "    class_idx = int(np.argmax(prediction_probs))\n",
    "    return class_names[class_idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cfef6d7-8b8b-440f-b0d4-77d475bf5736",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions\n",
    "\n",
    "train_data_predictions = loaded_best_model.predict(train_data, verbose=1)\n",
    "val_data_predictions = loaded_best_model.predict(val_data, verbose=1)\n",
    "test_data_predictions = loaded_best_model.predict(test_data, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f62723-3d33-4e36-bd53-bd396b775c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to unbatch the train_data to get the predictions, images and true labels\n",
    "train_data_images_unbatched_list = []\n",
    "train_data_labels_unbatched_list = []\n",
    "\n",
    "# loop through unbatched data\n",
    "for image, label in train_data.unbatch().as_numpy_iterator():\n",
    "    train_data_images_unbatched_list.append(image)\n",
    "    train_data_labels_unbatched_list.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac2f405-8f26-4ec8-93a8-85009e677f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to unbatch the val_data to get the predictions, images and true labels\n",
    "val_data_images_unbatched_list = []\n",
    "val_data_labels_unbatched_list = []\n",
    "\n",
    "# loop through unbatched data\n",
    "for image, label in val_data.unbatch().as_numpy_iterator():\n",
    "    val_data_images_unbatched_list.append(image)\n",
    "    val_data_labels_unbatched_list.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3348a83c-c1e6-45a2-b9d9-42b338af9a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to unbatch the test_data to get the predictions, images and true labels\n",
    "test_data_images_unbatched_list = []\n",
    "test_data_labels_unbatched_list = []\n",
    "\n",
    "# loop through unbatched data\n",
    "for image, label in test_data.unbatch().as_numpy_iterator():\n",
    "    test_data_images_unbatched_list.append(image)\n",
    "    test_data_labels_unbatched_list.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97445e80-d7b4-4671-8f99-5a14f1b181d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_unbatched_label = [get_pred_label(label) for label in train_data_labels_unbatched_list] # true label of validation data\n",
    "train_predictions_label = [get_pred_label(label) for label in train_data_predictions] # prediction on validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8bd175e-8a19-42cd-83c4-52fff7bdd130",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data_unbatched_label = [get_pred_label(label) for label in val_data_labels_unbatched_list] # true label of validation data\n",
    "val_predictions_label = [get_pred_label(label) for label in val_data_predictions] # prediction on validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fabc620-68bf-4448-9ba3-e415912506fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_unbatched_label = [get_pred_label(label) for label in test_data_labels_unbatched_list] # true label of validation data\n",
    "test_predictions_label = [get_pred_label(label) for label in test_data_predictions] # prediction on validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ff9dec-ee37-437b-a6fd-6883c250d7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataframes\n",
    "\n",
    "Train_Results_df = pd.DataFrame(\n",
    "    {\n",
    "        'True_Label': train_data_unbatched_label,\n",
    "        'Prediction_Label': train_predictions_label,\n",
    "        'Image_Data': train_data_images_unbatched_list,\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "Validation_Results_df = pd.DataFrame(\n",
    "    {\n",
    "        'True_Label': val_data_unbatched_label,\n",
    "        'Prediction_Label': val_predictions_label,\n",
    "        'Image_Data': val_data_images_unbatched_list,\n",
    "    }\n",
    ")\n",
    "\n",
    "Test_Results_df = pd.DataFrame(\n",
    "    {\n",
    "        'True_Label': test_data_unbatched_label,\n",
    "        'Prediction_Label': test_predictions_label,\n",
    "        'Image_Data': test_data_images_unbatched_list,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8146b3fb-2ca1-40c7-8084-1c806514ea7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show some rows of dataframe\n",
    "Train_Results_df[['True_Label', 'Prediction_Label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7a281f-6dee-4ffc-aa63-b49bcb75ca3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show some rows of dataframe\n",
    "Validation_Results_df[['True_Label', 'Prediction_Label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f81330a-8383-4c7b-86e8-9cbf770b59d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show some rows of dataframe\n",
    "Test_Results_df[['True_Label', 'Prediction_Label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e344f4eb-3a75-45f1-9fb9-b4ed593da9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Confusion Matrix\n",
    "\n",
    "train_cm = confusion_matrix(Train_Results_df['True_Label'], Train_Results_df['Prediction_Label'])\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=train_cm, display_labels=unique_labels)\n",
    "\n",
    "disp.plot(xticks_rotation=60)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5265ce77-e859-40e8-be09-e352165d4452",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation Confusion Matrix\n",
    "\n",
    "val_cm = confusion_matrix(Validation_Results_df['True_Label'], Validation_Results_df['Prediction_Label'])\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=val_cm, display_labels=unique_labels)\n",
    "\n",
    "disp.plot(xticks_rotation=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07975c8-115f-4f8b-b1d7-277a1f6b7cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Confusion Matrix\n",
    "\n",
    "test_cm = confusion_matrix(Test_Results_df['True_Label'], Test_Results_df['Prediction_Label'])\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=test_cm, display_labels=unique_labels)\n",
    "\n",
    "disp.plot(xticks_rotation=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c315e89-2bc0-4d40-a9d1-f723cec17457",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Classification Report\n",
    "\n",
    "train_report = classification_report(Train_Results_df['True_Label'], Train_Results_df['Prediction_Label'])\n",
    "print(train_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8747d5c5-73b0-4daa-b29d-b7e364a3df31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation Classification Report\n",
    "\n",
    "val_report = classification_report(Validation_Results_df['True_Label'], Validation_Results_df['Prediction_Label'])\n",
    "print(val_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad9fbd9-25c8-4547-ae7e-07a9419bf39b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Classification Report\n",
    "\n",
    "test_report = classification_report(Test_Results_df['True_Label'], Test_Results_df['Prediction_Label'])\n",
    "print(test_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead18180-f3a5-4a53-a2e6-de2c48d7980d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Figures Test Results (Plotly)\n",
    "\n",
    "# num_rows = np.ceil(np.sqrt(len(Test_Results_df))).astype(int)\n",
    "# num_cols = num_rows\n",
    "\n",
    "# df = Test_Results_df.head(100)  # shorthand, first 100 rows\n",
    "# n = len(df)\n",
    "# grid = int(np.ceil(np.sqrt(n)))\n",
    "\n",
    "# # Create list of subplot titles\n",
    "# titles = [\n",
    "#     f\"True Label: {row['True_Label']}<br>Predicted: {row['Prediction_Label']}\"\n",
    "#     for _, row in df.iterrows()\n",
    "# ]\n",
    "\n",
    "# # create figure\n",
    "# fig = make_subplots(rows=grid, cols=grid, subplot_titles=titles, vertical_spacing=0.02, horizontal_spacing=0.01)\n",
    "\n",
    "\n",
    "\n",
    "# def to_uint8(img):\n",
    "#     \"\"\"Ensure image is uint8 RGB for Plotly.\"\"\"\n",
    "#     arr = np.asarray(img)\n",
    "#     if arr.dtype == np.uint8:\n",
    "#         return arr\n",
    "#     # If floats in [0,1], scale up; otherwise clip to [0,255]\n",
    "#     if np.issubdtype(arr.dtype, np.floating) and arr.max() <= 1.0:\n",
    "#         arr = (np.clip(arr, 0.0, 1.0) * 255).astype(np.uint8)\n",
    "#     else:\n",
    "#         arr = np.clip(arr, 0, 255).astype(np.uint8)\n",
    "#     return arr\n",
    "\n",
    "# for idx, img in enumerate(df[\"Image_Data\"]):\n",
    "#     r = idx // grid + 1\n",
    "#     c = idx % grid + 1\n",
    "#     img_u8 = to_uint8(img)            # expects shape (H, W, 3)\n",
    "#     fig.add_trace(go.Image(z=img_u8), row=r, col=c)\n",
    "\n",
    "# # Hide ticks/axes and keep a nice square layout\n",
    "# fig.update_xaxes(visible=False)\n",
    "# fig.update_yaxes(visible=False)\n",
    "\n",
    "# # Optional: make figure size scale with grid\n",
    "# fig.update_layout(\n",
    "#     showlegend=False,\n",
    "#     #margin=dict(l=10, r=10, t=10, b=10),\n",
    "#     height=grid * 200,\n",
    "#     width=grid * 200,\n",
    "#     font=dict(size=2)\n",
    "# )\n",
    "\n",
    "# # Update the font size of all annotations (subplot titles)\n",
    "# fig.update_annotations(font_size=12) # Change 12 to your desired font size\n",
    "\n",
    "# fig.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaca7d27-06df-43dc-b375-d923b85aa145",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure using Matplotlib\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- Helper to ensure uint8 ---\n",
    "def to_uint8(img):\n",
    "    arr = np.asarray(img)\n",
    "    if arr.dtype == np.uint8:\n",
    "        return arr\n",
    "    if np.issubdtype(arr.dtype, np.floating) and arr.max() <= 1.0:\n",
    "        arr = (np.clip(arr, 0.0, 1.0) * 255).astype(np.uint8)\n",
    "    else:\n",
    "        arr = np.clip(arr, 0, 255).astype(np.uint8)\n",
    "    return arr\n",
    "\n",
    "# --- Setup Data ---\n",
    "df = Test_Results_df.head(100)\n",
    "n = len(df)\n",
    "grid = int(np.ceil(np.sqrt(n)))\n",
    "\n",
    "# --- Create Matplotlib Figure ---\n",
    "fig, axes = plt.subplots(grid, grid, figsize=(grid * 2, grid * 2))\n",
    "axes = axes.flatten()   # flatten for easy indexing\n",
    "\n",
    "for idx, (i, row) in enumerate(df.iterrows()):\n",
    "    ax = axes[idx]\n",
    "    img = to_uint8(row[\"Image_Data\"])\n",
    "    ax.imshow(img)\n",
    "    ax.set_title(\n",
    "        f\"True: {row['True_Label']}\\nPred: {row['Prediction_Label']}\",\n",
    "        fontsize=8\n",
    "    )\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "# Turn off any extra empty subplots\n",
    "for j in range(idx + 1, len(axes)):\n",
    "    axes[j].axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69cbf795-c5b9-4f0e-bd38-8da131027db1",
   "metadata": {},
   "source": [
    "# Test-Set Performance Evaluation\n",
    "\n",
    "Your model is a 4-class image classifier for **cataracts**, **diabetic retinopathy**, **glaucoma**, and **normal** retinal images.  \n",
    "Below is a structured interpretation of your test metrics.\n",
    "\n",
    "---\n",
    "\n",
    "## Overall Metrics\n",
    "- **Loss:** `0.3305`  \n",
    "- **Accuracy:** `0.8873`  \n",
    "- **Macro F1 Score:** `0.8840`\n",
    "\n",
    "**Interpretation:**  \n",
    "These are strong results for a 4-class medical image classification task. Accuracy near 0.89 is well above the 25% random baseline, and an F1 around 0.88 indicates balanced and reliable predictions across classes.\n",
    "\n",
    "---\n",
    "\n",
    "## Per-Class Precision\n",
    "\n",
    "| Class | Precision | Notes |\n",
    "|-------|-----------|-------|\n",
    "| **Cataract** | `0.9102` | Very strong; few false positives. |\n",
    "| **Diabetic Retinopathy** | `0.9727` | Excellent; highly reliable predictions. |\n",
    "| **Glaucoma** | `0.8161` | Weaker; more false positives. |\n",
    "| **Normal** | `0.8865` | Very good performance. |\n",
    "\n",
    "**Summary:**  \n",
    "Precision is strong overall, especially for diabetic retinopathy. Glaucoma is the only class showing moderate precision.\n",
    "\n",
    "---\n",
    "\n",
    "## Per-Class Recall\n",
    "\n",
    "| Class | Recall | Notes |\n",
    "|-------|--------|-------|\n",
    "| **Cataract** | `0.8889` | High sensitivity. |\n",
    "| **Diabetic Retinopathy** | `0.9596` | Outstanding recall. |\n",
    "| **Glaucoma** | `0.8023` | Lowest recall; misses some glaucoma cases. |\n",
    "| **Normal** | `0.8286` | Good, but could be improved. |\n",
    "\n",
    "**Summary:**  \n",
    "Recall is consistently high, but glaucoma remains the most challenging class.\n",
    "\n",
    "---\n",
    "\n",
    "## High-Level Assessment\n",
    "- **Performance is robust and clinically meaningful.**\n",
    "- The model reliably identifies all classes, especially diabetic retinopathy.\n",
    "- **Glaucoma** shows the weakest performance (expected due to subtle features).\n",
    "- No sign of extreme class imbalance or overfitting based on metrics alone.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "The model demonstrates **strong overall performance** with **Accuracy ~88.7%** and **Macro F1 ~88.4%**, delivering reliable detection across all four eye-condition classes.  \n",
    "Minor weaknesses in glaucoma detection are expected and can be improved with targeted techniques.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85721564-dce8-4609-90d2-9220b58d0d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a csv file of test_df with predictions\n",
    "timestamp = datetime.datetime.now().strftime('%Y%m%d-%H%M%S')\n",
    "test_results_output_df = test_df.copy()\n",
    "test_results_output_df['Predicted_Label'] = Test_Results_df['Prediction_Label'].values.tolist()\n",
    "output_csv_path = f\"test_results_with_predictions_tensorflow_{timestamp}.csv\"\n",
    "test_results_output_df.to_csv(output_csv_path, index=False)\n",
    "print(f\"Test results with predictions saved to {output_csv_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env003",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
